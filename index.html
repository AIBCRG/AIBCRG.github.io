<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neuromorphic Vision in Human-Robot Teaming</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #e0f7fa 0%, #b2ebf2 50%, #80deea 100%);
            min-height: 100vh;
        }

        nav {
            background: rgba(255, 255, 255, 0.95);
            padding: 15px 0;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            position: sticky;
            top: 0;
            z-index: 1000;
        }

        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: center;
            gap: 20px;
            padding: 0 20px;
        }

        .nav-btn {
            padding: 12px 30px;
            background: linear-gradient(135deg, #0288d1, #4fc3f7);
            color: white;
            border: none;
            border-radius: 25px;
            font-size: 1.1em;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 10px rgba(2, 136, 209, 0.3);
        }

        .nav-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 15px rgba(2, 136, 209, 0.4);
        }

        .nav-btn.active {
            background: linear-gradient(135deg, #01579b, #0277bd);
        }

        .section {
            display: none;
        }

        .section.active {
            display: block;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: rgba(255, 255, 255, 0.95);
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
            margin-bottom: 30px;
            text-align: center;
        }

        h1 {
            color: #0288d1;
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #666;
            font-size: 1.2em;
            font-weight: 300;
        }

        .intro-section {
            background: rgba(255, 255, 255, 0.95);
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
            margin-bottom: 30px;
        }

        .intro-section h2 {
            color: #0277bd;
            margin-bottom: 20px;
            font-size: 2em;
            border-bottom: 3px solid #4fc3f7;
            padding-bottom: 10px;
        }

        .intro-section p {
            margin-bottom: 15px;
            font-size: 1.1em;
            text-align: justify;
        }

        .highlight-box {
            background: linear-gradient(135deg, #b3e5fc, #e1f5fe);
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #0288d1;
            margin: 20px 0;
        }

        .applications-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
            margin-top: 30px;
        }

        .app-card {
            background: rgba(255, 255, 255, 0.95);
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.15);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .app-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.25);
        }

        .app-card h3 {
            color: #0288d1;
            margin-bottom: 15px;
            font-size: 1.5em;
            display: flex;
            align-items: center;
        }

        .app-card h3::before {
            content: "‚óè";
            color: #4fc3f7;
            margin-right: 10px;
            font-size: 1.2em;
        }

        .app-card p {
            color: #555;
            line-height: 1.8;
        }

        .key-features {
            background: rgba(255, 255, 255, 0.95);
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
            margin-top: 30px;
        }

        .key-features h2 {
            color: #0277bd;
            margin-bottom: 25px;
            font-size: 2em;
            text-align: center;
        }

        .features-list {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }

        .feature-item {
            background: linear-gradient(135deg, #e1f5fe, #b3e5fc);
            padding: 20px;
            border-radius: 10px;
            border-left: 3px solid #4fc3f7;
        }

        .feature-item h4 {
            color: #0288d1;
            margin-bottom: 10px;
        }

        .team-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 30px;
            margin-top: 30px;
        }

        .team-member {
            background: rgba(255, 255, 255, 0.95);
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.15);
            text-align: center;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .team-member:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.25);
        }

        .member-photo {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            object-fit: cover;
            margin: 0 auto 20px;
            border: 4px solid #4fc3f7;
            background: linear-gradient(135deg, #e1f5fe, #b3e5fc);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 3em;
            color: #0288d1;
        }

        .member-name {
            color: #0277bd;
            font-size: 1.5em;
            margin-bottom: 10px;
            font-weight: 600;
        }

        .member-role {
            color: #0288d1;
            font-size: 1.1em;
            margin-bottom: 15px;
            font-weight: 500;
        }

        .member-bio {
            color: #555;
            line-height: 1.6;
            margin-bottom: 15px;
        }

        .member-links {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-top: 15px;
        }

        .member-links a {
            color: #0288d1;
            text-decoration: none;
            font-size: 0.9em;
            padding: 8px 15px;
            border: 2px solid #4fc3f7;
            border-radius: 20px;
            transition: all 0.3s ease;
        }

        .member-links a:hover {
            background: #4fc3f7;
            color: white;
        }

        .publication-item {
            background: rgba(255, 255, 255, 0.95);
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
            margin-bottom: 20px;
            border-left: 4px solid #4fc3f7;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .publication-item:hover {
            transform: translateX(5px);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.15);
        }

        .pub-title {
            color: #0277bd;
            font-size: 1.3em;
            font-weight: 600;
            margin-bottom: 10px;
        }

        .pub-authors {
            color: #555;
            font-style: italic;
            margin-bottom: 8px;
        }

        .pub-venue {
            color: #0288d1;
            font-weight: 500;
            margin-bottom: 10px;
        }

        .pub-links {
            display: flex;
            gap: 10px;
            margin-top: 15px;
            flex-wrap: wrap;
        }

        .pub-links a {
            padding: 8px 16px;
            background: linear-gradient(135deg, #4fc3f7, #0288d1);
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.9em;
            transition: all 0.3s ease;
        }

        .pub-links a:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 10px rgba(2, 136, 209, 0.3);
        }

        .project-card {
            background: rgba(255, 255, 255, 0.95);
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.15);
            margin-bottom: 30px;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .project-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.25);
        }

        .project-header {
            display: flex;
            justify-content: space-between;
            align-items: start;
            margin-bottom: 20px;
            flex-wrap: wrap;
            gap: 15px;
        }

        .project-title {
            color: #0277bd;
            font-size: 1.8em;
            font-weight: 600;
            flex: 1;
        }

        .project-status {
            padding: 8px 20px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }

        .status-ongoing {
            background: linear-gradient(135deg, #81c784, #66bb6a);
            color: white;
        }

        .status-completed {
            background: linear-gradient(135deg, #64b5f6, #42a5f5);
            color: white;
        }

        .project-description {
            color: #555;
            line-height: 1.8;
            margin-bottom: 20px;
        }

        .project-details {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-top: 20px;
        }

        .project-detail-item {
            background: linear-gradient(135deg, #e1f5fe, #b3e5fc);
            padding: 15px;
            border-radius: 8px;
        }

        .project-detail-label {
            color: #0277bd;
            font-weight: 600;
            margin-bottom: 5px;
        }

        .project-detail-value {
            color: #555;
        }

        .year-divider {
            color: #0288d1;
            font-size: 1.8em;
            font-weight: 600;
            margin: 40px 0 25px 0;
            padding-bottom: 10px;
            border-bottom: 3px solid #4fc3f7;
        }

        footer {
            text-align: center;
            padding: 30px;
            color: white;
            margin-top: 40px;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 2em;
            }
            
            .applications-grid {
                grid-template-columns: 1fr;
            }

            .nav-container {
                flex-wrap: wrap;
            }

            .nav-btn {
                font-size: 1em;
                padding: 10px 20px;
            }
        }
    </style>
</head>
<body>
    <nav>
        <div class="nav-container">
            <button class="nav-btn active" onclick="showSection('home')">Home</button>
            <button class="nav-btn" onclick="showSection('projects')">Projects</button>
            <button class="nav-btn" onclick="showSection('publications')">Publications</button>
            <button class="nav-btn" onclick="showSection('team')">Team</button>
        </div>
    </nav>

    <div id="home" class="section active">
        <div class="container">
        <header>
            <h1>Neuromorphic/Event Vision for Human Robot Interaction/Collaboration</h1>
            <p class="subtitle">Advancing Human-Robot Interaction and Collaboration</p>
        </header>

        <div class="intro-section">
            <h2>What is Neuromorphic/Event Vision?</h2>
            <p>
                Neuromorphic vision, also known as event-based vision, represents a paradigm shift in how machines perceive the visual world. Unlike traditional frame-based cameras that capture images at fixed intervals, event cameras are bio-inspired sensors that mimic the human retina by asynchronously detecting changes in brightness at each pixel independently.
            </p>
            <p>
                These sensors generate "events" only when pixel-level brightness changes occur, resulting in a stream of asynchronous events with microsecond temporal resolution. This approach offers several compelling advantages: extremely high temporal resolution (up to 1 million events per second), high dynamic range (>120dB), low latency (<1ms), and significantly reduced data redundancy.
            </p>
            <div class="highlight-box">
                <strong>Key Innovation:</strong> Event cameras do not capture redundant information from static scenes, making them highly efficient for detecting motion and changes in dynamic environments‚Äîperfect for real-time human-robot interaction scenarios.
            </div>
        </div>

        <div class="intro-section">
            <h2>Applications in Human-Robot Interaction (HRI) and Collaboration (HRC)</h2>
            <p>
                The unique properties of neuromorphic vision make it exceptionally well-suited for human-robot interaction and collaboration tasks. The high temporal resolution and low latency enable robots to perceive and respond to human actions in real-time, while the low power consumption and robustness to lighting variations make these systems practical for deployment in diverse real-world environments.
            </p>
        </div>

        <div class="applications-grid">
            <div class="app-card">
                <h3>Gesture Recognition</h3>
                <p>
                    Event cameras excel at capturing fast hand and arm movements with precise temporal detail. This enables natural, intuitive gesture-based communication between humans and robots, allowing for touchless control interfaces and seamless command recognition even in challenging lighting conditions. The high temporal resolution captures subtle gesture nuances that traditional cameras might miss.
                </p>
            </div>

            <div class="app-card">
                <h3>Human Action Recognition</h3>
                <p>
                    Neuromorphic sensors can detect and classify complex human actions and activities in real-time by tracking motion patterns with microsecond precision. This capability is crucial for collaborative robots that need to understand what humans are doing to coordinate tasks effectively, ensure safety, and adapt their behavior to human workflows in shared workspaces.
                </p>
            </div>

            <div class="app-card">
                <h3>Facial Expression Recognition</h3>
                <p>
                    By capturing subtle facial muscle movements and micro-expressions with high temporal fidelity, event-based vision enables robots to perceive human emotional states and engagement levels. This emotional intelligence is essential for creating more empathetic and socially aware robots that can adapt their interaction style based on human affect and mental state.
                </p>
            </div>

            <div class="app-card">
                <h3>Human Trajectory Prediction</h3>
                <p>
                    The continuous, asynchronous nature of event data allows for highly accurate prediction of human movement trajectories. This is critical for collision avoidance, path planning, and proactive robot behavior in collaborative settings. Robots can anticipate where humans will move next and adjust their actions accordingly to maintain safe and efficient collaboration.
                </p>
            </div>

            <div class="app-card">
                <h3>Human Detection and Tracking</h3>
                <p>
                    Event cameras provide robust human detection and tracking capabilities even in fast motion scenarios, occlusions, and extreme lighting conditions. The high dynamic range and motion sensitivity ensure that humans are reliably detected, which is fundamental for safety-critical applications and maintaining awareness of all humans in the collaborative workspace.
                </p>
            </div>
        </div>

        <div class="key-features">
            <h2>Why Neuromorphic Vision for HRI/HRC?</h2>
            <div class="features-list">
                <div class="feature-item">
                    <h4>‚ö° Ultra-Low Latency</h4>
                    <p>Microsecond-level response time enables real-time interaction and immediate robot reaction to human actions.</p>
                </div>
                <div class="feature-item">
                    <h4>üîã Energy Efficiency</h4>
                    <p>Sparse event representation significantly reduces power consumption, enabling longer operation and mobile deployment.</p>
                </div>
                <div class="feature-item">
                    <h4>üåì High Dynamic Range</h4>
                    <p>Operates effectively in both bright sunlight and low-light conditions without adjustment or saturation.</p>
                </div>
                <div class="feature-item">
                    <h4>üéØ Motion Sensitivity</h4>
                    <p>Exceptional at detecting fast movements and subtle motions that are critical for human action understanding.</p>
                </div>
                <div class="feature-item">
                    <h4>üìä Data Efficiency</h4>
                    <p>Only captures relevant changes, reducing data processing requirements and bandwidth needs.</p>
                </div>
                <div class="feature-item">
                    <h4>üõ°Ô∏è Privacy Preserving</h4>
                    <p>Event-based representation inherently anonymizes static visual information while capturing motion dynamics.</p>
                </div>
            </div>
        </div>

        <footer>
            <p>&copy; 2025 Neuromorphic Vision Research | Advancing the Future of Human-Robot Interaction</p>
        </footer>
    </div>
    </div>

    <div id="projects" class="section">
        <div class="container">
            <header>
                <h1>Research Projects</h1>
                <p class="subtitle">Innovative Applications of Neuromorphic Vision</p>
            </header>

            <div class="intro-section">
                <h2>Current and Past Projects</h2>
                <p>
                    Our research group is actively working on several cutting-edge projects that push the boundaries of event-based vision for human-robot interaction and collaboration.
                </p>
            </div>

            <div class="project-card">
                <div class="project-header">
                    <h3 class="project-title">Real-Time Gesture Recognition for Collaborative Robots</h3>
                    <span class="project-status status-ongoing">Ongoing</span>
                </div>
                <p class="project-description">
                    This project develops a comprehensive gesture recognition system using event cameras for intuitive human-robot communication in manufacturing environments. The system can recognize complex hand gestures with millisecond latency, enabling workers to control collaborative robots naturally without physical interfaces. Our approach combines deep learning with event-based processing to achieve robust recognition even in challenging industrial lighting conditions.
                </p>
                <div class="project-details">
                    <div class="project-detail-item">
                        <div class="project-detail-label">Duration</div>
                        <div class="project-detail-value">2024 - Present</div>
                    </div>
                    <div class="project-detail-item">
                        <div class="project-detail-label">Funding</div>
                        <div class="project-detail-value">Research Council</div>
                    </div>
                    <div class="project-detail-item">
                        <div class="project-detail-label">Team Lead</div>
                        <div class="project-detail-value">Muhammad Hamza Zafar</div>
                    </div>
                </div>
            </div>

            <div class="project-card">
                <div class="project-header">
                    <h3 class="project-title">Event-Based Human Motion Prediction for Safe HRC</h3>
                    <span class="project-status status-ongoing">Ongoing</span>
                </div>
                <p class="project-description">
                    Focusing on safety-critical applications, this project develops algorithms for predicting human trajectories and intentions using neuromorphic vision sensors. By leveraging the high temporal resolution of event cameras, we can anticipate human movements up to 2 seconds in advance, allowing robots to proactively adjust their paths and avoid collisions. The system has been tested in shared workspace scenarios with promising results.
                </p>
                <div class="project-details">
                    <div class="project-detail-item">
                        <div class="project-detail-label">Duration</div>
                        <div class="project-detail-value">2023 - Present</div>
                    </div>
                    <div class="project-detail-item">
                        <div class="project-detail-label">Funding</div>
                        <div class="project-detail-value">EU Horizon</div>
                    </div>
                    <div class="project-detail-item">
                        <div class="project-detail-label">Team Lead</div>
                        <div class="project-detail-value">Syed Kumayl Raza Moosavi</div>
                    </div>
                </div>
            </div>

            <div class="project-card">
                <div class="project-header">
                    <h3 class="project-title">Neuromorphic Vision for Assistive Robotics</h3>
                    <span class="project-status status-ongoing">Ongoing</span>
                </div>
                <p class="project-description">
                    This project applies event-based vision to assistive robotics for elderly care and rehabilitation. We're developing systems that can detect falls, recognize activities of daily living, and understand user needs through natural gestures and movements. The low power consumption and privacy-preserving nature of event cameras make them ideal for continuous monitoring in home environments.
                </p>
                <div class="project-details">
                    <div class="project-detail-item">
                        <div class="project-detail-label">Duration</div>
                        <div class="project-detail-value">2024 - 2026</div>
                    </div>
                    <div class="project-detail-item">
                        <div class="project-detail-label">Funding</div>
                        <div class="project-detail-value">National Health Research</div>
                    </div>
                    <div class="project-detail-item">
                        <div class="project-detail-label">Team Lead</div>
                        <div class="project-detail-value">Dr. Filippo Sanfilippo</div>
                    </div>
                </div>
            </div>

            <div class="project-card">
                <div class="project-header">
                    <h3 class="project-title">Low-Latency Action Recognition Framework</h3>
                    <span class="project-status status-completed">Completed</span>
                </div>
                <p class="project-description">
                    Developed a novel deep learning architecture optimized for event-based action recognition with sub-10ms latency. The framework introduced innovative spatio-temporal convolution operations that efficiently process event streams while maintaining high accuracy. This work has been successfully deployed in industrial robotics applications and published in top-tier conferences.
                </p>
                <div class="project-details">
                    <div class="project-detail-item">
                        <div class="project-detail-label">Duration</div>
                        <div class="project-detail-value">2022 - 2024</div>
                    </div>
                    <div class="project-detail-item">
                        <div class="project-detail-label">Funding</div>
                        <div class="project-detail-value">Industry Partnership</div>
                    </div>
                    <div class="project-detail-item">
                        <div class="project-detail-label">Team Lead</div>
                        <div class="project-detail-value">Dr. Filippo Sanfilippo</div>
                    </div>
                </div>
            </div>

            <footer>
                <p>&copy; 2025 Neuromorphic Vision Research | Advancing the Future of Human-Robot Interaction</p>
            </footer>
        </div>
    </div>

    <div id="publications" class="section">
        <div class="container">
            <header>
                <h1>Publications</h1>
                <p class="subtitle">Our Research Contributions to Neuromorphic Vision</p>
            </header>

            <div class="intro-section">
                <h2>Selected Publications</h2>
                <p>
                    Our research team has published extensively in leading conferences and journals in computer vision, robotics, and human-robot interaction. Below is a selection of our recent publications.
                </p>
            </div>

            <div class="year-divider">2025</div>

            <div class="publication-item">
                <div class="pub-title">Ultra-Fast Gesture Recognition for Industrial Human-Robot Collaboration Using Event Cameras</div>
                <div class="pub-authors">Muhammad Hamza Zafar, Filippo Sanfilippo, Syed Kumayl Raza Moosavi</div>
                <div class="pub-venue">IEEE International Conference on Robotics and Automation (ICRA), 2025</div>
                <div class="pub-links">
                    <a href="#" target="_blank">PDF</a>
                    <a href="#" target="_blank">arXiv</a>
                    <a href="#" target="_blank">Code</a>
                    <a href="#" target="_blank">Video</a>
                </div>
            </div>

            <div class="publication-item">
                <div class="pub-title">Event-Based Human Trajectory Prediction with Temporal Attention Networks</div>
                <div class="pub-authors">Syed Kumayl Raza Moosavi, Filippo Sanfilippo, Muhammad Hamza Zafar</div>
                <div class="pub-venue">IEEE Transactions on Robotics (T-RO), 2025</div>
                <div class="pub-links">
                    <a href="#" target="_blank">PDF</a>
                    <a href="#" target="_blank">IEEE Xplore</a>
                    <a href="#" target="_blank">Code</a>
                </div>
            </div>

            <div class="year-divider">2024</div>

            <div class="publication-item">
                <div class="pub-title">Neuromorphic Vision for Real-Time Human Action Recognition in Collaborative Robotics</div>
                <div class="pub-authors">Filippo Sanfilippo, Muhammad Hamza Zafar, Syed Kumayl Raza Moosavi</div>
                <div class="pub-venue">International Journal of Computer Vision (IJCV), 2024</div>
                <div class="pub-links">
                    <a href="#" target="_blank">PDF</a>
                    <a href="#" target="_blank">DOI</a>
                    <a href="#" target="_blank">Supplementary</a>
                </div>
            </div>

            <div class="publication-item">
                <div class="pub-title">Low-Latency Facial Expression Recognition Using Event-Based Cameras for Empathetic Robots</div>
                <div class="pub-authors">Muhammad Hamza Zafar, Filippo Sanfilippo</div>
                <div class="pub-venue">ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2024</div>
                <div class="pub-links">
                    <a href="#" target="_blank">PDF</a>
                    <a href="#" target="_blank">ACM DL</a>
                    <a href="#" target="_blank">Poster</a>
                </div>
            </div>

            <div class="publication-item">
                <div class="pub-title">A Comprehensive Survey on Event-Based Vision for Human-Robot Interaction</div>
                <div class="pub-authors">Syed Kumayl Raza Moosavi, Muhammad Hamza Zafar, Filippo Sanfilippo</div>
                <div class="pub-venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024</div>
                <div class="pub-links">
                    <a href="#" target="_blank">PDF</a>
                    <a href="#" target="_blank">IEEE Xplore</a>
                </div>
            </div>

            <div class="year-divider">2023</div>

            <div class="publication-item">
                <div class="pub-title">Event-Driven Human Detection and Tracking for Dynamic Environments</div>
                <div class="pub-authors">Filippo Sanfilippo, Syed Kumayl Raza Moosavi</div>
                <div class="pub-venue">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023</div>
                <div class="pub-links">
                    <a href="#" target="_blank">PDF</a>
                    <a href="#" target="_blank">IEEE Xplore</a>
                    <a href="#" target="_blank">Code</a>
                </div>
            </div>

            <div class="publication-item">
                <div class="pub-title">Spatio-Temporal Attention for Event-Based Action Recognition</div>
                <div class="pub-authors">Muhammad Hamza Zafar, Syed Kumayl Raza Moosavi, Filippo Sanfilippo</div>
                <div class="pub-venue">Computer Vision and Pattern Recognition (CVPR), 2023</div>
                <div class="pub-links">
                    <a href="#" target="_blank">PDF</a>
                    <a href="#" target="_blank">arXiv</a>
                    <a href="#" target="_blank">Code</a>
                    <a href="#" target="_blank">Project Page</a>
                </div>
            </div>

            <footer>
                <p>&copy; 2025 Neuromorphic Vision Research | Advancing the Future of Human-Robot Interaction</p>
            </footer>
        </div>
    </div>

    <div id="team" class="section">
        <div class="container">
            <header>
                <h1>Our Team</h1>
                <p class="subtitle">Meet the Researchers Behind Neuromorphic Vision</p>
            </header>

            <div class="intro-section">
                <h2>Research Team</h2>
                <p>
                    Our interdisciplinary team brings together expertise in computer vision, robotics, neuromorphic engineering, and human-robot interaction to advance the state-of-the-art in event-based vision systems.
                </p>
            </div>

            <div class="team-grid">
                <div class="team-member">
                    <img src="filippo.jpeg" alt="Dr. Filippo Sanfilippo" class="member-photo">
                    <h3 class="member-name">Dr. Filippo Sanfilippo</h3>
                    <p class="member-role">Professor</p>
                    <p class="member-bio">
                        Leading research in neuromorphic vision systems with 10+ years of experience in event-based sensing and human-robot interaction.
                    </p>
                    <div class="member-links">
                        <a href="mailto:filippo.sanfilippo@uia.no">Email</a>
                        <a href="https://scholar.google.com/citations?user=i5FepLsAAAAJ&hl=no" target="_blank">Scholar</a>
                    </div>
                </div>

                <div class="team-member">
                    <img src="hamza.jpg" alt="Muhammad Hamza Zafar" class="member-photo">
                    <h3 class="member-name">Muhammad Hamza Zafar</h3>
                    <p class="member-role">PhD Researcher</p>
                    <p class="member-bio">
                        Specializes in gesture recognition and real-time event processing algorithms for collaborative robotics applications.
                    </p>
                    <div class="member-links">
                        <a href="mailto:muhammad.h.zafar@uia.no">Email</a>
                        <a href="https://scholar.google.com/citations?user=mehULisAAAAJ&hl=no&oi=ao" target="_blank">Scholar</a>
                    </div>
                </div>

                <div class="team-member">
                    <img src="kumayl.jpg" alt="Syed Kumayl Raza Moosavi" class="member-photo">
                    <h3 class="member-name">Syed Kumayl Raza Moosavi</h3>
                    <p class="member-role">PhD Researcher</p>
                    <p class="member-bio">
                        Working on human action recognition and trajectory prediction using event cameras for safe human-robot collaboration.
                    </p>
                    <div class="member-links">
                        <a href="mailto:syed.k.moosavi@uia.no">Email</a>
                        <a href="https://scholar.google.com/citations?user=dscAe68AAAAJ&hl=no" target="_blank">Scholar</a>
                    </div>
                </div>

                <div class="team-member">
                    <div class="member-photo">üë§</div>
                    <h3 class="member-name">Alex Chen</h3>
                    <p class="member-role">PhD Candidate</p>
                    <p class="member-bio">
                        Focuses on facial expression recognition and emotion detection using neuromorphic sensors for empathetic robots.
                    </p>
                    <div class="member-links">
                        <a href="#" target="_blank">Email</a>
                        <a href="#" target="_blank">LinkedIn</a>
                        <a href="#" target="_blank">Scholar</a>
                    </div>
                </div>

                <div class="team-member">
                    <div class="member-photo">üë§</div>
                    <h3 class="member-name">Sarah Johnson</h3>
                    <p class="member-role">Research Engineer</p>
                    <p class="member-bio">
                        Develops hardware-software integration for event-based vision systems in real-time robotic platforms.
                    </p>
                    <div class="member-links">
                        <a href="#" target="_blank">Email</a>
                        <a href="#" target="_blank">LinkedIn</a>
                        <a href="#" target="_blank">Scholar</a>
                    </div>
                </div>

                <div class="team-member">
                    <div class="member-photo">üë§</div>
                    <h3 class="member-name">Michael Brown</h3>
                    <p class="member-role">Master's Student</p>
                    <p class="member-bio">
                        Researching human detection and tracking algorithms for dynamic environments using event cameras.
                    </p>
                    <div class="member-links">
                        <a href="#" target="_blank">Email</a>
                        <a href="#" target="_blank">LinkedIn</a>
                        <a href="#" target="_blank">Scholar</a>
                    </div>
                </div>
            </div>

            <footer>
                <p>&copy; 2025 Neuromorphic Vision Research | Advancing the Future of Human-Robot Interaction</p>
            </footer>
        </div>
    </div>

    <script>
        function showSection(sectionId) {
            // Hide all sections
            document.querySelectorAll('.section').forEach(section => {
                section.classList.remove('active');
            });
            
            // Remove active class from all buttons
            document.querySelectorAll('.nav-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(sectionId).classList.add('active');
            
            // Add active class to clicked button
            event.target.classList.add('active');
            
            // Scroll to top
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }
    </script>
</body>
</html>












